function [meanpowexp] = tiervrule(r,n,ntiers,ntrials)%	TIERVRULE simulates variance power rule accumulation as function of r%		powexp = tiervrule(r,n,ntiers,ntrials)%		r is correlation between neurons%		n is number of neurons in the pools%		ntiers is the number of tiers.  In each tier there are%			n neurons with expected partial correlation of r%		ntrials is the number of trials used to estimate the variance%			on the final tier superneuron.%		(see comments in tiervrule.m)%%%%%%%%% We start with n neurons, and generate a common expected val%% and var/mean = 1.%% Set nsamples to a larger number if you want to inspect %% the distributions along the way (e.g., look at the variance%% on a particular neuron given its inputs on that tier%% This was important when I wanted to ensure that I had%% approximated poisson statistics at the neuron level%% In general the number of %% need N so that we can bootstrap to form the n neurons from vals that%% are all correlated% ntrials = 20;% n = 5;N = 2 * n;ntiers = 10;nsamples = 2;%	C = rancorrelmtx(N, r);	% this is really rmaxC = (~eye(N)) .* (r * ones(N,N)) + eye(N);Q = sqrtm(C);for i=1:ntrials  %% each sample begins with the same expected starting vals.  %% we use 36 because it is a reasonable number with a reasonable variance  %% and is unlikely to yield negative vals.  Consider 49.   x0 = 36 + 6 * randn(1,N);	  %% each x is a random deviate with expected value x0(i) and  %% variance = expected val.  %% hopefully x is positive; but we will be certain that V is.  % x = (sqrt(x0) .* randn(1,N)) + x0;  V = abs(x0);  for j = 1:ntiers	g1 = randn(N,1);	% next line is the desired covariance matrix%	S = (sqrt(V)' * sqrt(V)) .* C;%	Q = sqrtm(S);	%% next line gets unit variance 0 mean with desired correlation, 	%% to which we add the appropriate mean and variance 	% g2 = (Q * g1) + (V' * ones(1,nsamples));	%% generate a single long vector N by 1.	g2 = (sqrt(V') .* (Q * g1)) + (V');	%% Now, think about this.  g2 is the response from each of	%% N neurons.  We are only interested in n of these, chosen	%% at random from the larger set with replacement (bootstrap).	%% If this is the end of the simulation, then each member of pool	%% is a sample neuron which can be examined for its response	%% mean and variance.  If there are more tiers to go, then	%% we have to add up n neurons to get the input to the neuron	%% on the next tier.  But we want n neurons on the next tier.	%% So we take n random subsets of pool, each of size n.	%% Although we are only interested in n of these, we have to	%% continue to generate N so that we can bootstrap at the next tier	a = ceil(N * rand(N,N));	pool = ones(N);	pool(:) = g2(a(:));	%% the next line reports the mean r value	% mean((sum(corrcoef(pool'))-1)/(N-1))	%% the columns of pool should be partially correlated by r	% now we need means.  But we use only n cells to make the means.	% This is the actual implementation of the bootstrap.  We have 	% continued to represent N values in each column (randomly drawn	% with replacement from pool). But we only really want the 1st n of	% these.	m2 = mean(pool([1:n],:));		% look at the mean of the repetititions to ensure we're getting the vals we asked for.	% meanresp = mean(m2)	%% If we look at this point at the 	%% variance on a neuron (across the row) in the	%% tier across its repetitions, we would find var = mean	%% (uncomment next 2 lines to convince yourself)	%	powexp = log(std(g2').^2) ./ log(mean(g2'));	%	meanpowexp = mean(powexp)	%	%% now what are the expected values (and var) 	%% for the next set of n superneurons?	%% Answer: the mean value from a column of n neurons.  	%% We only need the 1st n; but we continue to compute all N	%% in order to carry through the bootstrap	V = abs(m2);  end  %% we have been through the tiers, so lets accumulate the responses  %% at this level.  superN(i,:) = pool(1:n);end%% the columns of superN represent the output from each of n neurons%% on ntrials (rows).  SInce the neurons should have identical mean%% and since they should behave identically, at this point in the %% model we can consider the n neurons just more samples on one neuron% mean(superN)% mean(superN(:))powexp = log(std(superN).^2) ./ log(mean(superN));meanpowexp = mean(powexp);