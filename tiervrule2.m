function [meanpowexp] = tiervrule2(Q,n,N,meanresp,gamma,ntiers,ntrials)%	TIERVRULE2 simulates variance power rule accumulation as function of r%		powexp = tiervrule(r,n,ntiers,ntrials)%		Q is the square root of the desired correlation matrix between neurons%			it must be 2*n by 2*n%		n is number of neurons in the pools%		N is the number of neurons from which n is drawn.  2*n is recommended.%		meanresp is the mean response of the original input neurons %		gamma is the ratio of variance to expected value of the %			neurons' response generator. (gamma=1 approximates%			Poisson generators from normal distributions)%		ntiers is the number of tiers.  In each tier there are%			n neurons with expected partial correlation of r%		ntrials is the number of trials used to estimate the variance%			on the final tier superneuron.%		(see comments in tiervrule2.m)%%%%%%%%% We start with n neurons, and generate a common expected val%% and var/mean = 1.%% Set nsamples to a larger number if you want to inspect %% the distributions along the way (e.g., look at the variance%% on a particular neuron given its inputs on that tier%% This was important when I wanted to ensure that I had%% approximated poisson statistics at the neuron level%% In general the number of %% need N so that we can bootstrap to form the n neurons from vals that%% are all correlatedfor i=1:ntrials  %% each sample begins with the same expected starting vals.  %% meanresp should be large enough to avoid negative vals.  %% Consider 49.   x = meanresp + sqrt(gamma * meanresp) * randn(1,N);	  %% each x is a random deviate with expected value meanresp and  %% variance = gamma * expected val.  %% hopefully x is positive; but we will be certain that V is by truncating  %% at 0.  V = halfrect(x);  for j = 1:ntiers	g1 = randn(N,1);	% unit variance 0-mean picks	%% next line gets unit variance 0 mean with desired correlation, 	%% to which we add the appropriate mean and variance 	%% generate a single long vector N by 1.	%% If gamma = 1, then the variance equals the mean, and we	%% are modelling poisson generators.	g2 = ((sqrt(gamma) * sqrt(V')) .* (Q * g1)) + (V');	%% Now, think about this.  g2 is the response from each of	%% N neurons.  We are only interested in n of these, chosen	%% at random from the larger set with replacement (bootstrap).	%% If this is the end of the simulation, then each member of pool	%% is a sample neuron which can be examined for its response	%% mean and variance.  If there are more tiers to go, then	%% we have to add up n neurons to get the input to the neuron	%% on the next tier.  But we want n neurons on the next tier.	%% So we take n random subsets of pool, each of size n.	%% Although we are only interested in n of these, we have to	%% continue to generate N so that we can bootstrap at the next tier	a = ceil(N * rand(N,N));	pool = ones(N);	pool(:) = g2(a(:));	%% the next line reports the mean r value	% mean((sum(corrcoef(pool'))-1)/(N-1))	%% the columns of pool should be partially correlated by r	% now we need means.  But we use only n cells to make the means.	% This is the actual implementation of the bootstrap.  We have 	% continued to represent N values in each column (randomly drawn	% with replacement from pool). But we only really want the 1st n of	% these.	m2 = mean(pool([1:n],:));		%% If we look at this point at the 	%% variance on a neuron (across the row) in the	%% tier across its repetitions, we would find var = gamma * mean	%% (uncomment next 2 lines to convince yourself)	%	powexp = log(std(g2').^2) ./ log(mean(g2'));	%	meanpowexp = mean(powexp)	%	%% now what are the expected values (and var) 	%% for the next set of n superneurons?	%% Answer: the mean value from a column of n neurons.  	%% We only need the 1st n; but we continue to compute all N	%% in order to carry through the bootstrap	V = abs(m2);  end  %% we have been through the tiers, so lets accumulate the responses  %% at this level. We might as well use all N units.  superN(i,:) = pool(1:N);end%% the columns of superN represent the output from each of n neurons%% on ntrials (rows).  SInce the neurons should have identical mean%% and since they should behave identically, at this point in the %% model we can consider the n neurons just more samples on one neuron% mean(superN)% mean(superN(:))powexp = log(std(superN).^2) ./ log(mean(superN));meanpowexp = mean(powexp);