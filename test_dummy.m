% test of dummy variables in regression.  I am trying to gain some% familiarity of the SS quantities in Draper and Smith.% from table 5.12 p.243data = [28 13.3 1  20 8.9 1  32 15.1 1  22 10.4 1  29 13.1 2  27 12.4 2  28 13.2 2  26 11.8 2  21 11.5 3  27 14.2 3  29 15.4 3  23 13.1 3  25 13.8 3]d = dummyvar(data(:,3))% this is different than Draper & Smith's.  We'll see if that matters.% It does, but removing the last column fixes thisd(:,3) = [];% The model is % y = b0 + b1*x * b2*z1 + b3*z2% where z1 and z2 are the dummy vars that are G:1,0 V:0,1 or W:0,0,% representing the Turkey origination. The book calls b2 and b3 a1 and a2.A = [ones(size(data,1),1) data(:,1) d][b bint r rint s] = regress(data(:,2), A, .05);% the mean square error is the sum of the square of the residuals divided by% the df.  The df is the number o fdata points minus the number of% parameters in the model.  That's the number or rows of data (or A) minus% the number of columns of A.ms2 = sum(r.^2) / (size(A,1)-size(A,2))diff(size(A))% for G, the fitted equation has fit [b(1)+b(3) b(2)]% for V,[b(1)+b(4) b(2)]% for W,[b(1) b(2)]% So far so good.  These eqs match the ones on p.245bsum(r.^2)				% residual SSssize(data)df_r = size(data,1)-size(A,2)s2 = sum(r.^2)/df_r			% MS residual% I want to make sure I understand the quantities that go into an ANOVA% table and the construction of the F statistic.%% b0. This is the model SS when the only thing in the model is the% constant.  That is just the mean value.m = mean(data(:,2))SSb0 = sum((m * ones(size(data(:,2)))).^2)% SSa1a2_b0.  This is the SS for the model with the dummy vars and the coef% but without the slope term.  It is the extra SS beyond what we had for b0% alone.  Hence the subtraction[ba12_b0 bint r rint s] = regress(data(:,2), A(:,[1 3 4]), .05); SSa1a2_b0 = sum((A(:,[1 3 4])*ba12_b0).^2) - SSb0% There are 2 degree of freedom here. so ms = SSa1a2_b0/2F = ms/ms2% we would look at F with df 2,9p = 1 - fcdf(F,2,9)% Next term is b1|b0,a1,a2.  Notice the shorthand for doing this. I use the% full model, and a lesser model, generate the SS for each using the norm()% function.  Norm is a peculiar matlab function in that it means different% things for matrices and vectors.  For vectors it is the sqrt of the sum of% squares.  So norm(x)^2 is the sum of the squares of vector x.b = regress(data(:,2),A);b0 = regress(data(:,2),A(:,[1 3 4]))msb = norm(A*b)^2 - norm(A(:,[1 3 4])*b0)^2 / 1 % note df=1p = 1-fcdf(msb/ms2,1,9)q1 = [1 1 1;1 0 0; 0 1 0; 0 0 0]q2 = [1 0 0; 1 1 1; 2 0 0; 2 1 2]rank(q1)rank(q2)help rankI assume this means the corrected sum of squares, corrected for the% grand mean.  Let's seesum((data(:,2)-m).^2)			% That is *not* correct% I Guess it must be the difference from the model prediction of b0sum((data(:,2)-b(1)).^2)		% nope% How about the model itself?sum((A*b).^2)				% closeb0b0 = b .* [1 0 1 1]'sum((A*b0).^2)b0 = regress[b0 bint r rint s] = regress(data(:,2),ones(size(data,1),1), .05); b0% So, b0 is the model SS when the only factor is the constant.  I'm not sure% why this isn't the mean.  It is the mean!  mean(data(:,2))sum((b0*ones(size(data,1),1)).^2)	% Got it!sum(b(1)*ones(size(data(:,2))).^2)% Total SS is easysum((data(:,2)).^2)