% fiddle with nested regression% start with my exchange with Brad Jones of mathworks% hope to spawn an mfileI have 12 observations of a response, y, and two inputs, x. The x's belong to3 groups with 5, 4, and 3 observations a piece (I want to make this prettygeneral.)% Brad writes, % Here is my code:g1 = ones(5,1);g2 = 2*ones(4,1);g3 = 3*ones(3,1);g = [g1;g2;g3];x = rand(12,2);d = dummyvar(g)   % dummyvar is a stats tbx function for creating "dummy"                   % variables for regression modeling.y = x*[3 4]'+d*[-3 -1 4]'+normrnd(0,.2,12,1);% $$$ % $$$ d =% $$$ % $$$      1     0     0% $$$      1     0     0% $$$      1     0     0% $$$      1     0     0% $$$      1     0     0% $$$      0     1     0% $$$      0     1     0% $$$      0     1     0% $$$      0     1     0% $$$      0     0     1% $$$      0     0     1% $$$      0     0     1[b,bint,r,rint,stats] = regress(y,[x d],0.05);% [b,bint,r,rint,stats] = regress(y,[x],0.05);b% $$$ stats% $$$ % $$$ % $$$ b =% $$$     ls est    true values% $$$     2.5528  ->  3% $$$     4.3368  ->  4% $$$    -2.9928  -> -3% $$$    -0.9685  -> -1% $$$     4.2344  ->  4% I'm not entirely sure that the difference between our approaches rests on% the issue of nested analyses, but I will look into this.  Meanwhile, I can% tell you why the approach you recommended does not work for my data set.% That leaves the original question of how to compute CI in a model in which% there is df penalty.  Save that and see if the example I generated makes% any sense to you (or tell me that you have no time for this and I will% go away).  %% In my data set I have effects of a variable, say size of an eye movement.% Call the variable AMP.  Among different groups I have different% characteristic values for AMP.  The average might vary from group to group% but the effect of the variation on AMP is similar from group to group.% The response, y, is greater if AMP is larger than average (for the% group).  I can see this if I standardize (z-transform) AMP so that it is% expressed in units of standard deviates with respect to its group mean. % % My goal is to perform a multivariate analysis of predictors, some like AMP% and some identically distributed within the groups.  The easiest way to do% this is to standardize and then perform regression.  Hence my original% question to you.  I think another way to achieve the same thing is to% perform a nested analysis -- but I'm fuzzy on this.% % I modified your example slightly to illustrate the point I am trying to% make. % Use 3 groups, just like you.  I needed to make them a little larger.g1 = ones(15,1);g2 = 2*ones(14,1);g3 = 3*ones(13,1);g = [g1;g2;g3];% I'm going to treat the 2 components of x separately, although I'll adopt% the same expected vals for the fit.%% For the first, x_1, we'll do exactly what you did. It is distributed in a% similar fashion among the groups.x_1 = rand(size(g,1),1);% This variable will contribute to the response as followsy_1 = x_1 * 3;				% the expected coef is 3.% For the second component of x, assume that the observations are not% identically distributed. For group 1, the mean of the *regressors* is% 1. For group 2 it is 3 and for group 3 it is 10.  The problem in Brad's% formulation is that he assumes the response is affected by x1 or x2 the% same way in each group.  The difference between groups is just a constant% factor.  But in my data, the values for x are different in the groups.% For example, say x2 is saccade amplitude.  I might havex12 = normrnd(1,sqrt(1),size(g1,1),1);x22 = normrnd(3,sqrt(3),size(g2,1),1);  x32 = normrnd(10,sqrt(10),size(g3,1),1);% Note that the variance scales with the mean, as is typical for my data and% yet another reason to standardize.x_2 = [x12;x22;x32];% The idea is that this variable contributes to the response according to% its variation from its mean, scaled by its stdev.z12 = (x12 - mean(x12)) / std(x12);	% standardized (z-tranform) xz22 = (x22 - mean(x22)) / std(x22);z32 = (x32 - mean(x32)) / std(x32);z_2 = [z12;z22;z32];y_2 = z_2 * 4;				% contribution of var2 to the responsed = dummyvar(g);			% distinguish the groups% The response is the sum of contributions from x_1, x_2 and the means from% each of the groups.  Brad's line is commmented out, followed by my% revision. % y = x*[3 4]'+d*[-3 -1 4]'+normrnd(0,.2,12,1);   % Brad'sy = y_1 + y_2 + d*[-3 -1 4]'+normrnd(0,.2,size(g,1),1); % mine% Here is what you get.  % If you do the regression respecting the group-wise z-tranform of x_2, you% get the values you're supposed to get [3 4 -3 -1 4].[b,bint,r,rint,stats] = regress(y,[x_1 z_2 d],0.05); b% $$$ % $$$ ans =% $$$ % $$$     3.1577    --> 3% $$$     3.9366    --> 4% $$$    -3.1908    --> -3% $$$    -1.1050    --> -1% $$$     3.9565    --> 4% If you run the model on the x vals (Brad's formulation) you don't expect% to get the same regression coefficients for variable2 or for unit offsets.[b,bint,r,rint,stats] = regress(y,[x_1 x_2 d],0.05); b% $$$ The answer is% $$$ b =% $$$ % $$$     2.0074  --> Bothersome!% $$$     1.4443  --> We no longer expect 4, but see below% $$$    -3.8973  --> No longer expect -3, but see below% $$$    -4.6578  --> etc.% $$$   -10.5648  % Of course we don't expect the same coef for x_2, but this val is% uninterpretable. The val from simple linear regression is 0.83onecol = ones(size(x_1,1),1);[b,bint,r,rint,stats] = regress(y,[onecol x_2],0.05); b  % The next three commands perform the same simple linear regression on the% individual groups.  I get 4.7, 2.7, 1.  We don't expect the same value% because I assumed a model in which the response scaled with the degree of% deviation in units of sdev.  But these numbers bear little resemblance to% the value from the multivariate regression with dummyvar.[b,bint,r,rint,stats] = regress(y(min(find(g==1)):max(find(g==1))),...    [ones(size(g1)) x12],0.05); b[b,bint,r,rint,stats] = regress(y(min(find(g==2)):max(find(g==2))),...    [ones(size(g2)) x22],0.05); b[b,bint,r,rint,stats] = regress(y(min(find(g==3)):max(find(g==3))),...    [ones(size(g3)) x32],0.05); b% While it is not surprising that the 2nd coefficient is off, it is not% obvious to me why the 1st coefficient is affected, but it is.  This is% especially bothersome.Mike,I am confused.We are trying to simulate nature here. What are "nature's variables" the x'sor the z's?If it is the x's, then we need the code below:x12 = normrnd(1,sqrt(1),size(g1,1),1);x22 = normrnd(3,sqrt(3),size(g2,1),1);  x32 = normrnd(10,sqrt(10),size(g3,1),1);x_2 = [x12;x22;x32];and y should be a function of x_2If it is the z's, then why bother with the x's at allz12 = (x12 - mean(x12)) / std(x12);	% standardized (z-tranform) xz22 = (x22 - mean(x22)) / std(x22);z32 = (x32 - mean(x32)) / std(x32);z_2 = [z12;z22;z32];z_2 is just a long way to generate a vector of standard normal values.y_2 = z_2 * 4;				% contribution of var2 to the responseIf the model isy = y_1 + y_2 + d*[-3 -1 4]'+normrnd(0,.2,size(g,1),1);Why am I supposed to regress with x_2 instead of z_2? It boils down to this... If you know z_2, use it for the regression.Some interesting alternative models are:% interaction between quantitative input and categorical inputy = y_1 + y_1.*g + d*[-3 -1 4]'+normrnd(0,.2,size(g,1),1);  % categorical variable is nested x12 = normrnd(1,sqrt(1),size(g1,1),1);x22 = normrnd(3,sqrt(3),size(g2,1),1);  x32 = normrnd(10,sqrt(10),size(g3,1),1);x_2 = [x12;x22;x32];y = y_1 + x_2 + normrnd(0,.2,size(g,1),1);One of the above may be what you want, or you can formulate another.Regards,Brad% mike fiddles with lscovhelp lscovx = [0:5]'y = 5 + 2*x + normrnd(zeros(size(x)),sqrt(x)); y(1)=5;A = [ones(size(x)) x]V = diag(x)% by playing with variance of the term at x=0, we can drive the fitted line% closer or further to an intercept of y(1)=5.V(1,1) = 1;lscov(A,y,V)regress(y,A)